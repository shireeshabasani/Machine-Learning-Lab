{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d3851db-6c37-4431-b401-c0018e753769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  target  \\\n",
      "0      I was wondering if anyone out there could enli...       7   \n",
      "1      A fair number of brave souls who upgraded thei...       4   \n",
      "2      well folks, my mac plus finally gave up the gh...       4   \n",
      "3      \\nDo you have Weitek's address/phone number?  ...       1   \n",
      "4      From article <C5owCB.n3p@world.std.com>, by to...      14   \n",
      "...                                                  ...     ...   \n",
      "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13   \n",
      "11310  I have a (very old) Mac 512k and a Mac Plus, b...       4   \n",
      "11311  I just installed a DX2-66 CPU in a clone mothe...       3   \n",
      "11312  \\nWouldn't this require a hyper-sphere.  In 3-...       1   \n",
      "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...       8   \n",
      "\n",
      "                       category  \n",
      "0                     rec.autos  \n",
      "1         comp.sys.mac.hardware  \n",
      "2         comp.sys.mac.hardware  \n",
      "3                 comp.graphics  \n",
      "4                     sci.space  \n",
      "...                         ...  \n",
      "11309                   sci.med  \n",
      "11310     comp.sys.mac.hardware  \n",
      "11311  comp.sys.ibm.pc.hardware  \n",
      "11312             comp.graphics  \n",
      "11313           rec.motorcycles  \n",
      "\n",
      "[11314 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df=pd.read_csv(\"/home/user/Downloads/nlp_train.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5133a89",
   "metadata": {},
   "source": [
    "## distinct (non-repeating) values from that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d13ee7b-f183-4fa0-850f-e74b09d30184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rec.autos', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space',\n",
       "       'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware',\n",
       "       'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc',\n",
       "       'misc.forsale', 'alt.atheism', 'sci.electronics', 'comp.windows.x',\n",
       "       'rec.sport.hockey', 'rec.sport.baseball', 'soc.religion.christian',\n",
       "       'talk.politics.mideast', 'talk.politics.misc', 'sci.crypt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d2981d2-0d66-4f6c-91fb-ee6dfe50e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a46adf",
   "metadata": {},
   "source": [
    "## access and print a specific value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9824e365-a31b-460f-bab8-bbdc2208f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0caec6d7-6394-4929-8cbf-8b5ae78fb8fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (40205181.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    vocabulary={I,Love,Programming,is,Interesting,pizza,very,much}\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#count Vectorisation\n",
    "d1=\"I Love Programming\"\n",
    "d2=\"Programming is Interesting\"\n",
    "d3=\"I Love pizza very very much\"\n",
    "vocabulary={I,Love,Programming,is,Interesting,pizza,very,much}\n",
    "d1=[1,1,1,0,0,0,0,0]\n",
    "d2=[0,0,1,1,1,0,0,0]\n",
    "d3=[1,1,0,0,0,1,2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfaf04b",
   "metadata": {},
   "source": [
    "## #count Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38f112ba-a8b2-4cd7-9734-cea06d282703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 2, 'programming': 5, 'is': 1, 'interesting': 0, 'pizza': 4, 'very': 6, 'much': 3}\n",
      "[[0 0 1 0 0 1 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [0 0 1 1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "d1=\"I Love Programming\"\n",
    "d2=\"Programming is Interesting\"\n",
    "d3=\"I Love pizza very very much\"\n",
    "documents=[d1,d2,d3]\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform(documents)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578040d",
   "metadata": {},
   "source": [
    "## CountVectorizer with: A fixed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124134e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['i' 'love' 'programming' 'is' 'interesting' 'pizza' 'very' 'much']\n",
      "d1: [1 1 1 0 0 0 0 0]\n",
      "d2: [0 0 1 1 1 0 0 0]\n",
      "d3: [1 1 0 0 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert all documents to lowercase manually before vectorizing\n",
    "documents = [\n",
    "    \"i love programming\",  \n",
    "    \"programming is interesting\",\n",
    "    \"i love pizza very very much\"\n",
    "]\n",
    "\n",
    "# Define the fixed vocabulary (all lowercase)\n",
    "fixed_vocab = [\"i\", \"love\", \"programming\", \"is\", \"interesting\", \"pizza\", \"very\", \"much\"]\n",
    "\n",
    "# Create a CountVectorizer instance with a fixed vocabulary and modified token pattern\n",
    "vectorizer = CountVectorizer(vocabulary=fixed_vocab, lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# Transform the documents into a bag-of-words representation\n",
    "X = vectorizer.transform(documents)\n",
    "\n",
    "# Convert the sparse matrix to an array for better readability\n",
    "bow_representation = X.toarray()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the CountVectorizer representation for each document\n",
    "for i, vector in enumerate(bow_representation):\n",
    "    print(f\"d{i+1}:\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1e350",
   "metadata": {},
   "source": [
    "## #2.import 20 news groups from scikit learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37563d0a-fa0a-4e14-bcfe-8ed16f166730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target  \\\n",
      "0     I am a little confused on all of the models of...       7   \n",
      "1     I'm not familiar at all with the format of the...       5   \n",
      "2                                   \\nIn a word, yes.\\n       0   \n",
      "3     \\nThey were attacking the Iraqis to drive them...      17   \n",
      "4     \\nI've just spent two solid months arguing tha...      19   \n",
      "...                                                 ...     ...   \n",
      "7527  \\n   Henry, if I read you correctly, you may b...      14   \n",
      "7528  about\\nthem on\\n\\nActually, I thought Macs wer...       4   \n",
      "7529  I sent a version of this post out a while ago,...       9   \n",
      "7530  I have this kit which includes the following :...       6   \n",
      "7531  \\nFine, but one of the points of this entire d...      15   \n",
      "\n",
      "                    category  \n",
      "0                  rec.autos  \n",
      "1             comp.windows.x  \n",
      "2                alt.atheism  \n",
      "3      talk.politics.mideast  \n",
      "4         talk.religion.misc  \n",
      "...                      ...  \n",
      "7527               sci.space  \n",
      "7528   comp.sys.mac.hardware  \n",
      "7529      rec.sport.baseball  \n",
      "7530            misc.forsale  \n",
      "7531  soc.religion.christian  \n",
      "\n",
      "[7532 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#2.import 20 news groups from scikit learn datasets\n",
    "df=pd.read_csv(\"/home/user/Downloads/nlp_test.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "637b5594-6099-4af6-bff2-599106c43bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target       category\n",
      "0     From: mccall@mksol.dseg.ti.com (fred j mccall ...       2      sci.space\n",
      "1     From: \"Changyaw Wang\" <wangc@cs.indiana.edu>\\n...       1  comp.graphics\n",
      "2     From: lioness@maple.circa.ufl.edu\\nSubject: Te...       1  comp.graphics\n",
      "3     From: hotopp@ami1.bwi.wec.com (Daniel T. Hotop...       1  comp.graphics\n",
      "4     From: Ad-Robot@bobsbox.rent.com (Robotic Posti...       1  comp.graphics\n",
      "...                                                 ...     ...            ...\n",
      "1097  From: malek@pi.titech.ac.jp (Zidouri Abdelmale...       1  comp.graphics\n",
      "1098  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0    alt.atheism\n",
      "1099  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...       0    alt.atheism\n",
      "1100  From: beck@irzr17.inf.tu-dresden.de (Andre Bec...       1  comp.graphics\n",
      "1101  From: 18084TM@msu.edu (Tom)\\nSubject: Billboar...       2      sci.space\n",
      "\n",
      "[1102 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/user/Downloads/newsgroups_test.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb494d37-ed6a-4a01-91d6-97bd6f9d6bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target       category\n",
      "0     From: degroff@netcom.com (21012d)\\nSubject: Re...       2      sci.space\n",
      "1     From: ab@nova.cc.purdue.edu (Allen B)\\nSubject...       1  comp.graphics\n",
      "2     From: healta@saturn.wwc.edu (Tammy R Healy)\\nS...       0    alt.atheism\n",
      "3     From: capelli@vnet.IBM.COM (Ron Capelli)\\nSubj...       1  comp.graphics\n",
      "4     From: henry@zoo.toronto.edu (Henry Spencer)\\nS...       2      sci.space\n",
      "...                                                 ...     ...            ...\n",
      "1652  From: ab@nova.cc.purdue.edu (Allen B)\\nSubject...       1  comp.graphics\n",
      "1653  From: renes@ecpdsharmony.cern.ch (Rene S. Dutc...       1  comp.graphics\n",
      "1654  From: xrcjd@resolve.gsfc.nasa.gov (Charles J. ...       2      sci.space\n",
      "1655  From: dietz@cs.rochester.edu (Paul Dietz)\\nSub...       2      sci.space\n",
      "1656  From: jhwitten@cs.ruu.nl (Jurriaan Wittenberg)...       2      sci.space\n",
      "\n",
      "[1657 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/user/Downloads/newsgroups_train.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa9fa9",
   "metadata": {},
   "source": [
    "## 9.print news training data of 5th article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d50ebaf9-c5da-4a3c-8e43-774ba5fac2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th Article in the Training Set:\n",
      "\n",
      "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n"
     ]
    }
   ],
   "source": [
    "#9.print news training data of 5th article\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the training subset of the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print the 5th article (index 4, since indexing starts at 0)\n",
    "print(\"5th Article in the Training Set:\\n\")\n",
    "print(newsgroups_train.data[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5ca34",
   "metadata": {},
   "source": [
    "## 11.print shape of the data and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed3aaac-9a8d-42a9-bf73-186a4fb327d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (data): 11314\n",
      "Number of target labels: 11314\n"
     ]
    }
   ],
   "source": [
    "#11.print shape of the data and targets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the training subset of the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print the shape of data and targets\n",
    "print(f\"Number of documents (data): {len(newsgroups_train.data)}\")\n",
    "print(f\"Number of target labels: {len(newsgroups_train.target)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa33cc",
   "metadata": {},
   "source": [
    "## 12.print training set file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cca2e66-773c-4773-b158-b209d7cef88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/user/scikit_learn_data/20news_home/20news-bydate-train/rec.autos/102994'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/51861'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/51879'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38242'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/sci.space/60880'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/talk.politics.guns/54525'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/sci.med/58080'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.ibm.pc.hardware/60249'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.os.ms-windows.misc/10008'\n",
      " '/home/user/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/50502']\n"
     ]
    }
   ],
   "source": [
    "#12.print training set file names\n",
    "print(newsgroups_train.filenames[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d309f",
   "metadata": {},
   "source": [
    "## 13.by using count vectorization train data into numerical format consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47c6035a-0411-4f32-86ff-fe085fffacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Count Vectorized Training Data: (11314, 101631)\n",
      "\n",
      "Sample vector representation of the first document:\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#13.by using count vectorization train data into numerical format consideration\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(f\"Shape of Count Vectorized Training Data: {x.shape}\")  # (num_documents, num_features)\n",
    "\n",
    "# Print a sample transformed vector (first document)\n",
    "print(\"\\nSample vector representation of the first document:\\n\")\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ca438",
   "metadata": {},
   "source": [
    "## #14.use bernouliNB for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deb84efc-87a0-48d5-8695-6f139ca67bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8530\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       319\n",
      "           1       0.73      0.96      0.83       389\n",
      "           2       0.96      0.75      0.84       394\n",
      "\n",
      "    accuracy                           0.85      1102\n",
      "   macro avg       0.88      0.85      0.86      1102\n",
      "weighted avg       0.88      0.85      0.85      1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#14.use bernouliNB for training\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = pd.read_csv(\"/home/user/Downloads/newsgroups_train.csv\")\n",
    "newsgroups_test = pd.read_csv(\"/home/user/Downloads/newsgroups_test.csv\")\n",
    "\n",
    "# Convert text data into binary numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)  # BernoulliNB requires binary features\n",
    "X_train = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "X_test = vectorizer.transform(newsgroups_test[\"text\"])\n",
    "\n",
    "# Define target labels\n",
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# Initialize and train Bernoulli Naïve Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6665985",
   "metadata": {},
   "source": [
    "## 15.by using count vectorization train data into numerical format consideration only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "242926b1-a6e3-497a-98ee-e3b0974f6f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Count Vectorized Training Data: (1657, 29663)\n",
      "\n",
      "Sample vector representation of the first document:\n",
      " ['00' '000' '0000' '00000' '000000' '000005102000' '000021'\n",
      " '000062david42' '000100255pixel' '000406']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#15.by using count vectorization train data into numerical format consideration only\n",
    "vectorizer=CountVectorizer()\n",
    "x = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "print(f\"Shape of Count Vectorized Training Data: {x.shape}\")  # (num_documents, num_features)\n",
    "\n",
    "# Print a sample transformed vector (first document)\n",
    "print(\"\\nSample vector representation of the first document:\\n\",vectorizer.get_feature_names_out()[:10])\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd4b12",
   "metadata": {},
   "source": [
    "## 16.predict target labels for testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5ee48c3-13c9-4741-b2f8-9afede7b552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target labels: 1102\n"
     ]
    }
   ],
   "source": [
    "#16.predict target labels for testing set\n",
    "print(f\"Number of target labels: {len(newsgroups_test.target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2dc73d",
   "metadata": {},
   "source": [
    "## test classification using Bernoulli navie bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7d2df8-765a-47c6-a693-5ac637cbb507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.9228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = pd.read_csv(\"/home/user/ML/database/newsgroups_test.csv\")\n",
    "newsgroups_test = pd.read_csv(\"/home/user/ML/database/newsgroups_train.csv\")\n",
    "\n",
    "# Convert text data into binary numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)  # Converts word presence (1) or absence (0)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "X_test = vectorizer.transform(newsgroups_test[\"text\"])\n",
    "\n",
    "# Define target labels\n",
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# Initialize and train Bernoulli Naïve Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792102c",
   "metadata": {},
   "source": [
    "##  text classification using the Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9afb67de-8a0e-40dd-a00d-4bf2b6ea5edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.9474\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       319\n",
      "           1       0.97      0.92      0.94       389\n",
      "           2       0.91      0.97      0.94       394\n",
      "\n",
      "    accuracy                           0.95      1102\n",
      "   macro avg       0.95      0.95      0.95      1102\n",
      "weighted avg       0.95      0.95      0.95      1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = pd.read_csv(\"/home/user/Downloads/newsgroups_train.csv\")\n",
    "newsgroups_test = pd.read_csv(\"/home/user/Downloads/newsgroups_test.csv\")\n",
    "\n",
    "# Convert text data into numerical format using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "X_test = vectorizer.transform(newsgroups_test[\"text\"])\n",
    "\n",
    "# Define target labels\n",
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# Initialize and train Multinomial Naïve Bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb775464",
   "metadata": {},
   "source": [
    "## FIND TEST SET ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd23a321-70c2-4c7b-8868-d78d12748ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# FIND TEST SET ACCURACY\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f814af",
   "metadata": {},
   "source": [
    "## # TRY WITH AVOIDING STOPWORDS AND TEPET THE SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34607a27-3e32-4925-8379-20ee0450e650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.9555\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       319\n",
      "           1       0.96      0.95      0.96       389\n",
      "           2       0.93      0.97      0.95       394\n",
      "\n",
      "    accuracy                           0.96      1102\n",
      "   macro avg       0.96      0.95      0.96      1102\n",
      "weighted avg       0.96      0.96      0.96      1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITH AVOIDING STOPWORDS AND TEPET THE SAME\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = pd.read_csv(\"/home/user/Downloads/newsgroups_train.csv\")\n",
    "newsgroups_test = pd.read_csv(\"/home/user/Downloads/newsgroups_test.csv\")\n",
    "\n",
    "# Convert text data into numerical format using TF-IDF Vectorizer with stopwords removal\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")  # Removes common stopwords\n",
    "X_train = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "X_test = vectorizer.transform(newsgroups_test[\"text\"])\n",
    "\n",
    "# Define target labels\n",
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# Initialize and train Multinomial Naïve Bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121153b",
   "metadata": {},
   "source": [
    "## find accuracy score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f54051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.9228\n"
     ]
    }
   ],
   "source": [
    " import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score  # ✅ Make sure this line is included\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = pd.read_csv(\"/home/user/ML/database/newsgroups_test.csv\")\n",
    "newsgroups_test = pd.read_csv(\"/home/user/ML/database/newsgroups_train.csv\")\n",
    "\n",
    "# Convert text data into binary numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train[\"text\"])\n",
    "X_test = vectorizer.transform(newsgroups_test[\"text\"])\n",
    "\n",
    "# Define target labels\n",
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# Initialize and train Bernoulli Naïve Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbabb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
